{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7e0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c589c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\91897\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\91897\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\91897\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fc146c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import time \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b005bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1586a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ad44f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name= input \n",
    "search_wbe=driver.find_element(By.CLASS_NAME,'nav-search-field ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a873282",
   "metadata": {},
   "source": [
    "Q1. Python program to search all the product under a particular product (guitar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f976f1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search: Guitar\n",
      "Kadence Guitar Acoustica Series, Electric Acoustic Guitar, Ash Wood with Pickup and Inbuilt tuner (Ash Wood, Electro Acoustic) with bag.\n",
      "Sichumaria 9 PCS Assorted Pearl Celluloid Guitar Picks 1.00mm\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Get user input for the product to search\n",
    "product = input(\"Enter the product to search: \")\n",
    "\n",
    "# Start the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to Amazon.in\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Find the search box and enter the product\n",
    "search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_box.send_keys(product)\n",
    "\n",
    "# Find the search button and click it\n",
    "search_button = driver.find_element(By.ID, \"nav-search-submit-button\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the search results to load\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Find all the product titles on the page\n",
    "product_titles = driver.find_elements(By.CSS_SELECTOR, \"span.a-size-medium.a-color-base.a-text-normal\")\n",
    "\n",
    "# Print the titles of the products\n",
    "for title in product_titles:\n",
    "    print(title.text)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb9766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba5cc57",
   "metadata": {},
   "source": [
    "Q2. To scrape the following details of each product listed in first 3 pages of your search \n",
    "results and save it in a data frame and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7fab72a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search: Guitars\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"a[aria-label='Next']\"}\n  (Session info: chrome=114.0.5735.111)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x002CA813+48355]\n\t(No symbol) [0x0025C4B1]\n\t(No symbol) [0x00165358]\n\t(No symbol) [0x001909A5]\n\t(No symbol) [0x00190B3B]\n\t(No symbol) [0x001BE232]\n\t(No symbol) [0x001AA784]\n\t(No symbol) [0x001BC922]\n\t(No symbol) [0x001AA536]\n\t(No symbol) [0x001882DC]\n\t(No symbol) [0x001893DD]\n\tGetHandleVerifier [0x0052AABD+2539405]\n\tGetHandleVerifier [0x0056A78F+2800735]\n\tGetHandleVerifier [0x0056456C+2775612]\n\tGetHandleVerifier [0x003551E0+616112]\n\t(No symbol) [0x00265F8C]\n\t(No symbol) [0x00262328]\n\t(No symbol) [0x0026240B]\n\t(No symbol) [0x00254FF7]\n\tBaseThreadInitThunk [0x770A7D59+25]\n\tRtlInitializeExceptionChain [0x77E0B74B+107]\n\tRtlClearBits [0x77E0B6CF+191]\n\t(No symbol) [0x00000000]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     products\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of the Product\u001b[39m\u001b[38;5;124m\"\u001b[39m: title,\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m\"\u001b[39m: price,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct URL\u001b[39m\u001b[38;5;124m\"\u001b[39m: url\n\u001b[0;32m     59\u001b[0m     })\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Check if there are more pages to navigate to\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m next_button \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma[aria-label=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_button\u001b[38;5;241m.\u001b[39mis_enabled():\n\u001b[0;32m     64\u001b[0m     next_button\u001b[38;5;241m.\u001b[39mclick()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:831\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    828\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    829\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    438\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"a[aria-label='Next']\"}\n  (Session info: chrome=114.0.5735.111)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x002CA813+48355]\n\t(No symbol) [0x0025C4B1]\n\t(No symbol) [0x00165358]\n\t(No symbol) [0x001909A5]\n\t(No symbol) [0x00190B3B]\n\t(No symbol) [0x001BE232]\n\t(No symbol) [0x001AA784]\n\t(No symbol) [0x001BC922]\n\t(No symbol) [0x001AA536]\n\t(No symbol) [0x001882DC]\n\t(No symbol) [0x001893DD]\n\tGetHandleVerifier [0x0052AABD+2539405]\n\tGetHandleVerifier [0x0056A78F+2800735]\n\tGetHandleVerifier [0x0056456C+2775612]\n\tGetHandleVerifier [0x003551E0+616112]\n\t(No symbol) [0x00265F8C]\n\t(No symbol) [0x00262328]\n\t(No symbol) [0x0026240B]\n\t(No symbol) [0x00254FF7]\n\tBaseThreadInitThunk [0x770A7D59+25]\n\tRtlInitializeExceptionChain [0x77E0B74B+107]\n\tRtlClearBits [0x77E0B6CF+191]\n\t(No symbol) [0x00000000]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Get user input for the product to search\n",
    "product = input(\"Enter the product to search: \")\n",
    "\n",
    "# Start the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to Amazon.in\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Find the search box and enter the product\n",
    "search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_box.send_keys(product)\n",
    "\n",
    "# Find the search button and click it\n",
    "search_button = driver.find_element(By.ID, \"nav-search-submit-button\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the search results to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"span.a-size-medium.a-color-base.a-text-normal\")))\n",
    "\n",
    "# Scrape the details of each product in the search results\n",
    "products = []\n",
    "pages = 0\n",
    "\n",
    "while pages < 3:\n",
    "    # Find all the product titles on the page\n",
    "    product_titles = driver.find_elements(By.CSS_SELECTOR, \"span.a-size-medium.a-color-base.a-text-normal\")\n",
    "    product_urls = driver.find_elements(By.CSS_SELECTOR, \"a.a-link-normal.a-text-normal\")\n",
    "    product_prices = driver.find_elements(By.CSS_SELECTOR, \"span.a-price-whole\")\n",
    "    product_returns = driver.find_elements(By.XPATH, \"//span[contains(text(), 'Return')]\")\n",
    "    product_delivery = driver.find_elements(By.XPATH, \"//span[contains(text(), 'Delivery')]\")\n",
    "    product_availability = driver.find_elements(By.XPATH, \"//span[contains(text(), 'Available') or contains(text(), 'In stock')]\")\n",
    "    \n",
    "    for i in range(len(product_titles)):\n",
    "        # Extract the text for each detail, replacing missing values with '-'\n",
    "        title = product_titles[i].text\n",
    "        url = product_urls[i].get_attribute(\"href\")\n",
    "        price = product_prices[i].text if i < len(product_prices) else \"-\"\n",
    "        returns = product_returns[i].text if i < len(product_returns) else \"-\"\n",
    "        delivery = product_delivery[i].text if i < len(product_delivery) else \"-\"\n",
    "        availability = product_availability[i].text if i < len(product_availability) else \"-\"\n",
    "        \n",
    "        # Append the details to the products list\n",
    "        products.append({\n",
    "            \"Name of the Product\": title,\n",
    "            \"Price\": price,\n",
    "            \"Return/Exchange\": returns,\n",
    "            \"Expected Delivery\": delivery,\n",
    "            \"Availability\": availability,\n",
    "            \"Product URL\": url\n",
    "        })\n",
    "\n",
    "    # Check if there are more pages to navigate to\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"a[aria-label='Next']\")\n",
    "    if next_button.is_enabled():\n",
    "        next_button.click()\n",
    "        pages += 1\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"span.a-size-medium.a-color-base.a-text-normal\")))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the products list\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"product_details.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991fcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cf07a10",
   "metadata": {},
   "source": [
    "Q3. Python program to access the search bar and search button on images.google.com and scrape 10 \n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6905c225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for 'fruits':\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRSkVHstQ7XuGE2J3aPtTkYV6jU5p179g31OQ&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHHgIwS2o8_ErQZraxmA2XT0qphlSd89wqLA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSlTxho4ctAuGF119v5lR4Qe1Xfj7l13gg7Cw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR7oSoUCq0CC_O-cc_Rhg5fXPv4JR1R5b8uVw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQVdLpI0Gi8gGCZXvgKjZJGCm0ATCxsd19Qsg&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ41V_ROSOjcx0H6FKJRfgrXZFqObOY1cywsg&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTfarAFccmDz22ZlfrL_vWW9Rxja-7INLgIRA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyWb-q6BU9OulA0_Sd50IR3FvIgKuoDoyLoA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQHTCFoDgSf0YM2KTCt07Ny1eA8J6fxPphiug&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0nZ9eNQFi3BajkoMYVquFA5qlyJmhRLA-ew&usqp=CAU\n",
      "\n",
      "Images for 'cars':\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcToVaqLoq1_Bjr937gz-fu9hCHGSebBXg0nRA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTe8kx59jo4rI5rswab3fiPwxFY9sKkLuulmw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS1-RODkP1Wkuj8XQO1ifV87HSQHOTL6aoOMg&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQvJg2StZfMMNJGATpLUrPKewNB8BN6ApYAFA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSURGRQ8m0KbpZLV25-9DZQWIidxvRSvvsB1Q&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRugQQ6VZjNX58oivBNN_gJFh7mWQD4IguqVA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTbsibbJfofgIuCp5GN4K0zUnOY3BpTaBnemQ&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRAdFQeBr_RoZRSS7V8Do_np1g-Nn258aKlTw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQHBAu0UM5V2WQ-7fN2NomllKKc1hrQ70xekg&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTj5ofcC9hMHitDnxpVSDjoc9BL6PaQ3hSPLw&usqp=CAU\n",
      "\n",
      "Images for 'Machine Learning':\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQEH9HPwKldOAGZirNcAK8bzgLto484e7pqmw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHwO_w6wJ5YQntNGkOd5jYoKThMXr4LAewew&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSCINhdYd9WN9Q9uLYnNmN1o209gwlYnWA6_w&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4xhOrVpxNBBsHLNix52gQXESTjBp0WynSuw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRlEPje7tm8hNYh01jxphNYcbkGsYnjiCutbQ&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRT4Nhjak0705OdNdPIAZTc-xyskXn57Dvyxw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzfQdj9y43pLeC0ZxH0XcZWRWVjEQGAlTcjQ&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSOjWgRelw7O_fjohYuyXeJzFd0uxbJWrUAlw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcThYZsP3hhKXjbOt-Uja1UQnZ4nTBaulCGJgA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQPsqjPEZcc8N3ux0RMpr3L9TkudMUavVy9MQ&usqp=CAU\n",
      "\n",
      "Images for 'Guitar':\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQwY8thsZ-ehqXd4FGSC738LD7nOZ0tMrL66g&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQijF5TYelTGsO5Io-F30VC5yhDj563iRu5_w&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ129Ov0qMWjJJk0rp0n785XGwAMAqSNj90zA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTWdqwh-qAr8pzeUDy7v5VAvflrguv-AcEzGw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSABnfAGSHH4qgvx42dezPuZnaIpiWStTaZTA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRrd3q4fd8smxGa0dRgA2Fbb0W5uHLnlRomfw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4JEAHAMcnmEVVc4oN56G9T41Eh7jL18xkug&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRgyA7qP_nnC6pFn3HHWjq791qeJTMZTWTnsA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQBinQFhhFPEtfSMBfVRSJrrYuPGn5tuzdL1w&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQPxUb652fFsjbxbRqFYXQbCOamb9lAE0UxjA&usqp=CAU\n",
      "\n",
      "Images for 'Cakes':\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSeqbR0--GS3vZN02Yi4ca7B2xIDGXhbW2mig&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTEOPgELXA7y0MKGlmbPWSqtUTNO04GNuYMsQ&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS-MLE0biBSn6zhJEsxHM7unPyK-CcUL9EtEA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTw4FLqjJjj3gLDB20SKYttlvOZcCXB_KVa-Q&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTmSNbpMlu1-zSRc5-n7j0lZ3SXmeCVdI0ZCw&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSOcgmoDg32ZxRDJ-2oLcnm8HBYi-uFaUnbTg&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTbngjFw7cGhkPYydLabFXvg0f4CSDMFpLdwQ&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSn3UdUT5wErGsk-5Qzs0ZLP-DdV0-fs1wiJA&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQp8mPwh-WXJQnf-wYvv_x6NW1_fe1YOzmXdg&usqp=CAU\n",
      "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqQzYHeVXWOfh3bDaFBQnSQClE22-c3xRQpA&usqp=CAU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Function to scrape images for a given keyword\n",
    "def scrape_images(driver, keyword):\n",
    "    # Clear the search bar\n",
    "    search_bar = driver.find_element(\"name\", \"q\")\n",
    "    search_bar.clear()\n",
    "\n",
    "    # Enter the keyword in the search bar\n",
    "    search_bar.send_keys(keyword)\n",
    "\n",
    "    # Press Enter to perform the search\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the search results to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scroll down to load more images (optional)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Find all image elements\n",
    "    image_elements = driver.find_elements(\"css selector\", \"img.rg_i\")\n",
    "\n",
    "    # Scrape the URLs of the images\n",
    "    image_urls = []\n",
    "    for image in image_elements:\n",
    "        try:\n",
    "            src = image.get_attribute(\"src\")\n",
    "            if src.startswith(\"http\"):\n",
    "                image_urls.append(src)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    # Return the scraped image URLs\n",
    "    return image_urls[:10]  # Return only the first 10 images\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a new Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open images.google.com\n",
    "    driver.get(\"https://images.google.com\")\n",
    "\n",
    "    # Keywords to search for\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "    # Scrape images for each keyword\n",
    "    for keyword in keywords:\n",
    "        image_urls = scrape_images(driver, keyword)\n",
    "        print(f\"Images for '{keyword}':\")\n",
    "        for url in image_urls:\n",
    "            print(url)\n",
    "        print()\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41519c2",
   "metadata": {},
   "source": [
    "Q4. Python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand \n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32eb629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape smartphone details from Flipkart\n",
    "def scrape_smartphones(keyword):\n",
    "    # Create a new Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open Flipkart website\n",
    "    driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "    # Close login popup if present\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//button[contains(text(),'✕')]\").click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Search for the keyword\n",
    "    search_box = driver.find_element(By.CSS_SELECTOR, \"input[name='q']\")\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.submit()\n",
    "\n",
    "    # Wait for the search results to load\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Find all product cards\n",
    "    product_cards = driver.find_elements(By.CSS_SELECTOR, \"div._1AtVbE\")\n",
    "\n",
    "    results = []\n",
    "    for card in product_cards:\n",
    "        try:\n",
    "            brand_name = card.find_element(By.CSS_SELECTOR, \"div._4rR01T\").text.strip()\n",
    "        except:\n",
    "            brand_name = \"-\"\n",
    "\n",
    "        try:\n",
    "            smartphone_name = card.find_element(By.CSS_SELECTOR, \"a.IRpwTa\").text.strip()\n",
    "        except:\n",
    "            smartphone_name = \"-\"\n",
    "\n",
    "        try:\n",
    "            color = card.find_element(By.CSS_SELECTOR, \"a._1WKKIW\").text.strip()\n",
    "        except:\n",
    "            color = \"-\"\n",
    "\n",
    "        try:\n",
    "            details = card.find_element(By.CSS_SELECTOR, \"ul._1xgFaf\")\n",
    "            ram = details.find_element(By.XPATH, \".//*[contains(text(), 'RAM')]/following-sibling::li\").text.strip()\n",
    "            rom = details.find_element(By.XPATH, \".//*[contains(text(), 'Internal Storage')]/following-sibling::li\").text.strip()\n",
    "            primary_camera = details.find_element(By.XPATH, \".//*[contains(text(), 'Primary Camera')]/following-sibling::li\").text.strip()\n",
    "            secondary_camera = details.find_element(By.XPATH, \".//*[contains(text(), 'Secondary Camera')]/following-sibling::li\").text.strip()\n",
    "            display_size = details.find_element(By.XPATH, \".//*[contains(text(), 'Display Size')]/following-sibling::li\").text.strip()\n",
    "            battery_capacity = details.find_element(By.XPATH, \".//*[contains(text(), 'Battery Capacity')]/following-sibling::li\").text.strip()\n",
    "        except:\n",
    "            ram = \"-\"\n",
    "            rom = \"-\"\n",
    "            primary_camera = \"-\"\n",
    "            secondary_camera = \"-\"\n",
    "            display_size = \"-\"\n",
    "            battery_capacity = \"-\"\n",
    "\n",
    "        try:\n",
    "            price = card.find_element(By.CSS_SELECTOR, \"div._30jeq3._1_WHN1\").text.strip()\n",
    "        except:\n",
    "            price = \"-\"\n",
    "\n",
    "        try:\n",
    "            product_url = card.find_element(By.CSS_SELECTOR, \"a.IRpwTa\").get_attribute(\"href\")\n",
    "        except:\n",
    "            product_url = \"-\"\n",
    "\n",
    "        result = {\n",
    "            \"Brand Name\": brand_name,\n",
    "            \"Smartphone Name\": smartphone_name,\n",
    "            \"Colour\": color,\n",
    "            \"RAM\": ram,\n",
    "            \"Storage(ROM)\": rom,\n",
    "            \"Primary Camera\": primary_camera,\n",
    "            \"Secondary Camera\": secondary_camera,\n",
    "            \"Display Size\": display_size,\n",
    "            \"Battery Capacity\": battery_capacity,\n",
    "            \"Price\": price,\n",
    "            \"Product URL\": product_url,\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Keyword to search for\n",
    "    keyword = \"Oneplus Nord\"\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e4b3d",
   "metadata": {},
   "source": [
    "Q5.Python program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf09bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "City = input('Enter City name that has to be searched : ')\n",
    "search_bar = driver.find_element_by_id('searchboxinput')\n",
    "search_bar.click()\n",
    "time.sleep(2)\n",
    "\n",
    "#sending keys to find cities\n",
    "search_bar.send_keys(City)\n",
    "\n",
    "#checking for webelement and clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"searchbox-searchbutton\")\n",
    "search_btn.click()\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    url_str = driver.current_url\n",
    "    print(\"URL Extracted: \", url_str)\n",
    "    latitude_longitude = re.findall(r'@(.*)data',url_str)\n",
    "    if len(latitude_longitude):\n",
    "        lat_lng_list = latitude_longitude[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            latitude = lat_lng_list[0]\n",
    "            longitude = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(latitude, longitude))\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))\n",
    "        \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fdee46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62c17431",
   "metadata": {},
   "source": [
    "Q6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def scrape_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "    # Configure Selenium to use a web driver (e.g., Chrome)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Load the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Find the section containing the laptop details\n",
    "    laptops_section = driver.find_element_by_class_name('top-sellers')\n",
    "\n",
    "    # Find all the individual laptop divs\n",
    "    laptop_divs = laptops_section.find_elements_by_class_name('TopNumbeHeading')\n",
    "\n",
    "    # Iterate over each laptop div and extract the details\n",
    "    for laptop_div in laptop_divs:\n",
    "        # Extract the laptop name\n",
    "        laptop_name = laptop_div.find_element_by_tag_name('a').text.strip()\n",
    "\n",
    "        # Extract the laptop specifications\n",
    "        specs_div = laptop_div.find_element_by_xpath('./following-sibling::div[@class=\"TopTechSpecs\"]')\n",
    "        specifications = specs_div.text.strip()\n",
    "\n",
    "        # Extract the laptop price\n",
    "        price_div = laptop_div.find_element_by_xpath('./following-sibling::div[@class=\"smprice\"]')\n",
    "        price = price_div.text.strip()\n",
    "\n",
    "        # Print the laptop details\n",
    "        print(\"Laptop Name:\", laptop_name)\n",
    "        print(\"Specifications:\", specifications)\n",
    "        print(\"Price:\", price)\n",
    "        print()\n",
    "\n",
    "    # Close the web driver\n",
    "    driver.quit()\n",
    "\n",
    "# Call the function to start scraping\n",
    "scrape_laptops()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6992b",
   "metadata": {},
   "source": [
    "Q7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd744dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode to avoid opening a browser window\n",
    "\n",
    "    # Set up the Chrome driver executable path\n",
    "    chrome_driver_path = \"path/to/chromedriver\"  # Replace with the actual path to chromedriver executable\n",
    "\n",
    "    # Create a new Chrome driver instance\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        # Load the URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Find the table containing the billionaires' details\n",
    "        table = driver.find_element(By.CLASS_NAME, \"table\")\n",
    "\n",
    "        # Find all the table rows except the header row\n",
    "        rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]\n",
    "\n",
    "        # Iterate over each row and extract the details\n",
    "        for row in rows:\n",
    "            # Extract the data cells\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "\n",
    "            # Extract the details from the cells\n",
    "            rank = cells[0].text.strip()\n",
    "            name = cells[1].text.strip()\n",
    "            net_worth = cells[2].text.strip()\n",
    "            age = cells[3].text.strip()\n",
    "            citizenship = cells[4].text.strip()\n",
    "            source = cells[5].text.strip()\n",
    "            industry = cells[6].text.strip()\n",
    "\n",
    "            # Print the billionaire details\n",
    "            print(\"Rank:\", rank)\n",
    "            print(\"Name:\", name)\n",
    "            print(\"Net Worth:\", net_worth)\n",
    "            print(\"Age:\", age)\n",
    "            print(\"Citizenship:\", citizenship)\n",
    "            print(\"Source:\", source)\n",
    "            print(\"Industry:\", industry)\n",
    "            print()\n",
    "\n",
    "    finally:\n",
    "        # Quit the driver\n",
    "        driver.quit()\n",
    "\n",
    "# Call the function to start scraping\n",
    "scrape_billionaires()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a2e14",
   "metadata": {},
   "source": [
    "Q8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "# opening the youtube.com\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# finding element for search bar\n",
    "search_bar = driver.find_element_by_xpath(\"//div[@class='ytd-searchbox-spt']/input\")\n",
    "search_bar.send_keys(\"GOT\")      # entering video name\n",
    "time.sleep(2)\n",
    "\n",
    "#clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")\n",
    "search_btn.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# clicking on first video\n",
    "video = driver.find_element_by_xpath(\"//yt-formatted-string[@class='style-scope ytd-video-renderer']\")\n",
    "video.click()\n",
    "\n",
    "# 1000 times we scroll down by 10000 in order to generate more comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")\n",
    "\n",
    "# creating empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []\n",
    "\n",
    "# scrape comments\n",
    "cm = driver.find_elements_by_id(\"content-text\")\n",
    "for i in cm:\n",
    "    if i.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(i.text)\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "# scrape time when comment was posted\n",
    "tm = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "for i in tm:\n",
    "    Time.append(i.text)\n",
    "    \n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "# scrape the comment likes\n",
    "like = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43792e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe for scraped data\n",
    "\n",
    "Youtube = pd.DataFrame({})\n",
    "Youtube['Comment'] = comments[:500]\n",
    "Youtube['Comment Time'] = comment_time[:500]\n",
    "Youtube['Comment Upvotes'] = No_of_Likes[:500]\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e5ec8",
   "metadata": {},
   "source": [
    "Q9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94514f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels():\n",
    "    url = \"https://www.hostelworld.com/\"\n",
    "\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode to avoid opening a browser window\n",
    "\n",
    "    # Set up the Chrome driver executable path\n",
    "    chrome_driver_path = \"path/to/chromedriver\"  # Replace with the actual path to chromedriver executable\n",
    "\n",
    "    # Create a new Chrome driver instance\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        # Load the URL\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "        # Select the search input and enter \"London\"\n",
    "        search_input = driver.find_element(By.ID, \"location-text-input-field\")\n",
    "        search_input.send_keys(\"London\")\n",
    "\n",
    "        # Click the search button\n",
    "        search_button = driver.find_element(By.CSS_SELECTOR, \".button.primary\")\n",
    "        search_button.click()\n",
    "        time.sleep(5)  # Wait for the search results to load\n",
    "\n",
    "        # Get the page source after dynamic content has loaded\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        # Find the list of hostels\n",
    "        hostels = soup.select(\".property-card\")\n",
    "\n",
    "        # Iterate over each hostel and extract the details\n",
    "        for hostel in hostels:\n",
    "            hostel_name = hostel.select_one(\".property-card__title\").get_text(strip=True)\n",
    "            distance = hostel.select_one(\".property-card__distance\").get_text(strip=True)\n",
    "            ratings = hostel.select_one(\".score\").get_text(strip=True)\n",
    "            total_reviews = hostel.select_one(\".reviews\").get_text(strip=True)\n",
    "            overall_reviews = hostel.select_one(\".keyword\").get_text(strip=True)\n",
    "            privates_price = hostel.select_one(\".price\").get_text(strip=True)\n",
    "            dorms_price = hostel.select_one(\".price.title-6\").get_text(strip=True)\n",
    "            facilities = \", \".join([item.get_text(strip=True) for item in hostel.select(\".facilities-label\")])\n",
    "            description = hostel.select_one(\".property-card__description\").get_text(strip=True)\n",
    "\n",
    "            # Print the hostel details\n",
    "            print(\"Hostel Name:\", hostel_name)\n",
    "            print(\"Distance:\", distance)\n",
    "            print(\"Ratings:\", ratings)\n",
    "            print(\"Total Reviews:\", total_reviews)\n",
    "            print(\"Overall Reviews:\", overall_reviews)\n",
    "            print(\"Privates from Price:\", privates_price)\n",
    "            print(\"Dorms from Price:\", dorms_price)\n",
    "            print(\"Facilities:\", facilities)\n",
    "            print(\"Description:\", description)\n",
    "            print()\n",
    "\n",
    "    finally:\n",
    "        # Quit the driver\n",
    "        driver.quit()\n",
    "\n",
    "# Call the function to start scraping\n",
    "scrape_hostels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ffe42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
